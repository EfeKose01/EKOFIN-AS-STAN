# create_index.py - GÃœNCELLENMÄ°Åž SÃœRÃœM
import os
import json
import pickle
from glob import glob
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
from typing import List, Dict

# --- YENÄ° EKLENEN: Scraper ModÃ¼lÃ¼ ---
import scraper  # scraper.py dosyasÄ±nÄ± import ediyoruz

# --- YapÄ±landÄ±rma ---
SOURCE_FILES_GLOB = [
    "jsonl_out/*.jsonl",  # DÃœZELTME: KlasÃ¶rÃ¼n iÃ§indeki .jsonl dosyalarÄ±
    "datalar_extracted/**/*.md",
    "datalar_extracted/**/*.txt"
]
FAISS_INDEX_PATH = "rag_index.faiss"
CONTENT_MAP_PATH = "rag_content.pkl"
EMBEDDING_MODEL = 'paraphrase-multilingual-mpnet-base-v2'
MAX_CHUNK_CHARS = 1000
OVERLAP = 150


def read_and_chunk_files() -> List[Dict[str, str]]:
    """TÃ¼m kaynak dosyalarÄ± okur ve metin parÃ§alarÄ±na (chunk) ayÄ±rÄ±r."""
    chunks = []
    paths = []
    # Glob desenlerini geniÅŸlet
    for pattern in SOURCE_FILES_GLOB:
        paths.extend(glob(pattern, recursive=True))

    print(f"ðŸ“‚ Taranacak dosya sayÄ±sÄ±: {len(paths)}")

    for path in paths:
        # print(f"-> Ä°ÅŸleniyor: {path}") # Ã‡ok dosya varsa konsolu kirletmesin diye kapattÄ±m
        try:
            full_text = ""
            if path.endswith('.jsonl'):
                with open(path, 'r', encoding='utf-8') as f:
                    for line in f:
                        if not line.strip(): continue
                        data = json.loads(line)
                        full_text += data.get('content', '') + "\n"
            else:
                with open(path, "r", encoding="utf-8", errors="ignore") as f:
                    full_text = f.read()

            if not full_text.strip(): continue

            start = 0
            while start < len(full_text):
                end = start + MAX_CHUNK_CHARS
                chunk_text = full_text[start:end]
                if chunk_text.strip():
                    # Metadata iÃ§in dosya yolunu da ekliyoruz
                    chunks.append({
                        "path": path,
                        "text": chunk_text,
                        "tokens": chunk_text.lower().split()
                    })
                start += MAX_CHUNK_CHARS - OVERLAP
        except Exception as e:
            print(f"HATA: {path} dosyasÄ± iÅŸlenemedi - {e}")
            continue
    return chunks


def build_and_save_index():
    # 1. ADIM: Ã–nce yeni verileri kontrol et ve indir
    print("ðŸŒ Resmi Gazete gÃ¼ncellemeleri kontrol ediliyor...")
    scraper.fetch_daily_resmi_gazete()
    print("------------------------------------------------")

    # 2. ADIM: Ä°ndeksleme
    print("Anlamsal indeksleme baÅŸlÄ±yor...")
    content_chunks = read_and_chunk_files()
    if not content_chunks:
        print("UYARI: Ä°ndekslenecek iÃ§erik bulunamadÄ±.")
        return

    print(f"ðŸ“¦ Toplam {len(content_chunks)} adet metin parÃ§asÄ± (chunk) iÅŸleniyor.")

    print(f"ðŸ§  '{EMBEDDING_MODEL}' modeli yÃ¼kleniyor...")
    model = SentenceTransformer(EMBEDDING_MODEL)

    print("ðŸ”¢ Metinler vektÃ¶rlere dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor (Bu iÅŸlem biraz sÃ¼rebilir)...")
    texts_to_encode = [chunk["text"] for chunk in content_chunks]
    embeddings = model.encode(texts_to_encode, show_progress_bar=True)

    embedding_dimension = embeddings.shape[1]
    index = faiss.IndexFlatL2(embedding_dimension)
    index.add(np.array(embeddings).astype('float32'))

    print(f"ðŸ’¾ '{FAISS_INDEX_PATH}' dosyasÄ±na kaydediliyor...")
    faiss.write_index(index, FAISS_INDEX_PATH)

    print(f"ðŸ’¾ '{CONTENT_MAP_PATH}' dosyasÄ±na kaydediliyor...")
    with open(CONTENT_MAP_PATH, 'wb') as f:
        pickle.dump(content_chunks, f)

    print("\nâœ… SÄ°STEM GÃœNCELLENDÄ° VE Ä°NDEKS BAÅžARIYLA OLUÅžTURULDU!")


if __name__ == "__main__":
    build_and_save_index()